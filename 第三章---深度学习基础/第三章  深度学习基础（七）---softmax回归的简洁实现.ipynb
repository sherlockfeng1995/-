{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 softmax回归的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.1 获取和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.2 定义和初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 回归的输出层是一个全连接层，所以我们用一个线性模块就可以了。因为前面我们数据返回的每个batch样本x的形状为(batch_size, 1, 28, 28), 所以我们要先用 view()将x的形状转换成(batch_size, 784)才送入全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=784\n",
    "num_outputs=10\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear=nn.Linear(num_inputs,num_outputs)\n",
    "    def forward(self,x): #x shape:(batch,1,28,28)\n",
    "        y=self.linear(x.view(x.shape[0],-1))\n",
    "        return y \n",
    "net=LinearNet(num_inputs,num_outputs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将对x的形状转换的这个功能自定义一个FlattenLayer并记录在d2lzh_pytorch中方便后面使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用nn.Sequential 搭建网络，Sequential是有序容器。网络层将按照在传入Sequential的顺序依次被添加到计算图中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (flatten): FlattenLayer()\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0010,  0.0273, -0.0051,  ..., -0.0028,  0.0262,  0.0083],\n",
      "        [-0.0255, -0.0110,  0.0096,  ..., -0.0072, -0.0285, -0.0322],\n",
      "        [-0.0191,  0.0177, -0.0349,  ..., -0.0231, -0.0057,  0.0244],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0349, -0.0279,  ..., -0.0260, -0.0208, -0.0297],\n",
      "        [ 0.0017, -0.0107, -0.0057,  ...,  0.0300,  0.0073,  0.0058],\n",
      "        [-0.0175, -0.0054, -0.0231,  ..., -0.0009, -0.0231, -0.0198]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0341, -0.0022, -0.0195,  0.0226,  0.0188, -0.0326, -0.0317,  0.0012,\n",
      "        -0.0049,  0.0255], requires_grad=True)] [torch.Size([10, 784]), torch.Size([10])]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict    #有序字典\n",
    "net=nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('flatten',FlattenLayer()),\n",
    "        ('linear',nn.Linear(num_inputs,num_outputs))\n",
    "    ])\n",
    ")\n",
    "# softmax(torch.mm(X.view(-1,num_inputs),W)+b)\n",
    "print(net)\n",
    "print([param for param in net.parameters()],[param.shape for param in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "#使用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数\n",
    "\n",
    "init.normal_(net.linear.weight,mean=0,std=0.01)\n",
    "init.constant_(net.linear.bias,val=0)\n",
    "print(net.linear.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.3 softmax和交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分开定义softmax运算和交叉熵损失函数会造成数值不稳定\n",
    "\n",
    "loss=nn.CrossEntropyLoss()#softmax运算和交叉熵损失计算的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.4 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#学习率为0.1的小批量随机梯度下降\n",
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.5 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.747, test acc 0.787\n",
      "epoch 2, loss 0.0022, train acc 0.811, test acc 0.806\n",
      "epoch 3, loss 0.0021, train acc 0.826, test acc 0.812\n",
      "epoch 4, loss 0.0020, train acc 0.833, test acc 0.818\n",
      "epoch 5, loss 0.0019, train acc 0.837, test acc 0.754\n"
     ]
    }
   ],
   "source": [
    "num_epochs=5\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
