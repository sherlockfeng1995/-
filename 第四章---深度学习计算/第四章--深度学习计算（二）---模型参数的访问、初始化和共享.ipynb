{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([2, 1])\n",
      "tensor(1.0375, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#init模块，包含多种模型初始化方法。\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "net=nn.Sequential(\n",
    "    nn.Linear(4,3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3,1)\n",
    ")\n",
    "\n",
    "print(net)\n",
    "X=torch.rand(2,4)\n",
    "print(net(X).size())\n",
    "Y=net(X).sum()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 访问模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential类与Module类的继承关系。对于Sequential实例中含模型参数的层，我们可以通过Module类的parameters()或者named_parameters方法来访问所有参数（以迭代器的形式返回），后者除了返回参数Tensor外还会返回其名字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "#访问多层感知机net的所有参数\n",
    "\n",
    "print(type(net.named_parameters()))#为什么是generator类型\n",
    "for name,param in net.named_parameters():#自动加'weight','bias'的name名\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于使用Sequential类构造的神经网络，我们可以通过方括号[]来访问网络的任一层。索引0表示隐藏层为Sequential实例最先添加的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "#访问net中单层的参数\n",
    "\n",
    "for name,param in net[0].named_parameters():\n",
    "    print(name,param.size(),type(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>param的类型为torch.nn.parameter.Parameter，其实这是Tensor的子类，和Tensor不同的是如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyModel,self).__init__(**kwargs)\n",
    "        self.weight1=nn.Parameter(torch.rand(20,20))#测试是否被加到模型参数列表中\n",
    "        self.weight2=torch.rand(20,20)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        pass\n",
    "\n",
    "n=MyModel()\n",
    "for name,param in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为Parameter是Tensor，即Tensor拥有的属性它都有，比如可以根据data来访问参数数值，用grad来访问参数梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4962, -0.3997, -0.4672,  0.1563],\n",
      "        [ 0.3101,  0.4098, -0.3128, -0.2650],\n",
      "        [ 0.4419, -0.0384, -0.3100, -0.2552]])\n",
      "None\n",
      "tensor([[-0.2597, -0.0914, -0.1951, -0.2985],\n",
      "        [ 0.0952,  0.0431,  0.0751,  0.0902],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "weight_0=list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad)#反向传播前梯度为None\n",
    "Y.backward()#为什么不是根据误差进行反向传播\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0136,  0.0115, -0.0144,  0.0140],\n",
      "        [ 0.0184, -0.0013, -0.0006,  0.0176],\n",
      "        [-0.0006,  0.0092, -0.0066, -0.0019]])\n",
      "2.weight tensor([[ 0.0080, -0.0057,  0.0037]])\n"
     ]
    }
   ],
   "source": [
    "#将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，将偏差参数清零\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight'in name:\n",
    "        init.normal_(param,mean=0,std=0.01)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#使用常数初始化偏置参数\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param,val=0)#设置偏置项为常数项\n",
    "        print(name,param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 自定义初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.init.normal_初始化方法\n",
    "def normal_(tensor,mean=0,std=0.01):\n",
    "    with torch.no_grad():#with 语法用于简化资源操作的后续清除操作\n",
    "        return tensor.normal_(mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:param\n",
      " 0.weight Parameter containing:\n",
      "tensor([[-6.2914,  7.6446,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000, -9.9715],\n",
      "        [ 6.6579,  7.6042,  5.7890, -0.0000]], requires_grad=True)\n",
      "name:param\n",
      " 2.weight Parameter containing:\n",
      "tensor([[-0., 0., -0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 令权重有一半概率初始化为0，\n",
    "# 有另一半概率初始化为[−10,−5]和[5,10]两个区间里均匀分布的随机数。\n",
    "\n",
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10,10)\n",
    "        tensor*=(tensor.abs()>=5).float()#当绝对值大于5时：tensor*1，小于5时：tensor*0\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print('name:param\\n',name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#还可通过改变这些参数的data来改写模型参数值，同时不会影响梯度\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data +=1\n",
    "        print(name,param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 共享模型参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何共享模型参数: Module类的forward函数里多次调用同一个层。此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight Parameter containing:\n",
      "tensor([[3.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear=nn.Linear(1,1,bias=False)\n",
    "net=nn.Sequential(\n",
    "    linear,\n",
    "    linear)\n",
    "print(net)\n",
    "for name,param in net.named_parameters():\n",
    "    init.constant_(param,val=3)\n",
    "    print(name,param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#在内存中，两个线性层是一个对象\n",
    "\n",
    "print(id(net[0])==id(net[1]))\n",
    "\n",
    "print(id(net[0].weight)==id(net[1].weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "#因为模型参数里包含了梯度，在反向传播计算时，共享参数的梯度是累加的\n",
    "\n",
    "x=torch.ones(1,1)\n",
    "y=net(x).sum()\n",
    "print('y:',y)\n",
    "y.backward()\n",
    "print(net[0].weight.grad)#单次梯度是3，两次就是6\n",
    "print(net[1].weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
